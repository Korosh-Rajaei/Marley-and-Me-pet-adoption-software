{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/ymoslem/OpenNMT-Tutorial/blob/main/2-NMT-Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"vSUyCs23M_H2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728044216532,"user_tz":-120,"elapsed":221687,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}},"outputId":"8cb0656f-5031-4c99-d722-9b2e4fcdddef"},"source":["# Install OpenNMT-py 3.x\n","!pip3 install OpenNMT-py"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting OpenNMT-py\n","  Downloading OpenNMT_py-3.5.1-py3-none-any.whl.metadata (8.8 kB)\n","Collecting torch<2.3,>=2.1 (from OpenNMT-py)\n","  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","Collecting configargparse (from OpenNMT-py)\n","  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n","Collecting ctranslate2<5,>=4 (from OpenNMT-py)\n","  Downloading ctranslate2-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.17.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.5)\n","Collecting waitress (from OpenNMT-py)\n","  Downloading waitress-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n","Collecting pyonmttok<2,>=1.37 (from OpenNMT-py)\n","  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (6.0.2)\n","Collecting sacrebleu (from OpenNMT-py)\n","  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rapidfuzz (from OpenNMT-py)\n","  Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Collecting pyahocorasick (from OpenNMT-py)\n","  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n","Collecting fasttext-wheel (from OpenNMT-py)\n","  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.7.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.16.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->OpenNMT-py) (71.0.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->OpenNMT-py) (1.26.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.64.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.7)\n","Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3,>=2.1->OpenNMT-py)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3,>=2.1->OpenNMT-py)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3,>=2.1->OpenNMT-py)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3,>=2.1->OpenNMT-py)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3,>=2.1->OpenNMT-py)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3,>=2.1->OpenNMT-py)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3,>=2.1->OpenNMT-py)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.2.0 (from torch<2.3,>=2.1->OpenNMT-py)\n","  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3,>=2.1->OpenNMT-py) (12.6.68)\n","Collecting pybind11>=2.2 (from fasttext-wheel->OpenNMT-py)\n","  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (2.2.0)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (8.1.7)\n","Collecting portalocker (from sacrebleu->OpenNMT-py)\n","  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2024.9.11)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n","Collecting colorama (from sacrebleu->OpenNMT-py)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (4.9.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.12.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (4.66.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.9.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (24.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.4.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3,>=2.1->OpenNMT-py) (2.1.5)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->OpenNMT-py) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (2.23.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->OpenNMT-py) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->OpenNMT-py) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->OpenNMT-py) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->OpenNMT-py) (2024.8.30)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->OpenNMT-py) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->OpenNMT-py) (0.1.5)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (13.8.1)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->OpenNMT-py) (0.19.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->OpenNMT-py) (7.0.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3,>=2.1->OpenNMT-py) (1.3.0)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->OpenNMT-py) (1.2.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->OpenNMT-py) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (0.1.2)\n","Downloading OpenNMT_py-3.5.1-py3-none-any.whl (262 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m262.8/262.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ctranslate2-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.2 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m37.2/37.2 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n","Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading waitress-3.0.0-py3-none-any.whl (56 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.7/56.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n","Installing collected packages: waitress, triton, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ctranslate2, configargparse, colorama, sacrebleu, nvidia-cusolver-cu12, nvidia-cudnn-cu12, fasttext-wheel, torch, OpenNMT-py\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.23.4\n","    Uninstalling nvidia-nccl-cu12-2.23.4:\n","      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.3.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.3.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.3.3\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.6.59\n","    Uninstalling nvidia-cufft-cu12-11.2.6.59:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.6.59\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.68\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.68:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.68\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.68\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.68:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.68\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.1.4\n","    Uninstalling nvidia-cublas-cu12-12.6.1.4:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.1.4\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.4.69\n","    Uninstalling nvidia-cusolver-cu12-11.6.4.69:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.4.69\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.4.0.58\n","    Uninstalling nvidia-cudnn-cu12-9.4.0.58:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.4.0.58\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.4.1+cu121\n","    Uninstalling torch-2.4.1+cu121:\n","      Successfully uninstalled torch-2.4.1+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.2.2 which is incompatible.\n","torchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed OpenNMT-py-3.5.1 colorama-0.4.6 configargparse-1.7 ctranslate2-4.4.0 fasttext-wheel-0.9.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 portalocker-2.10.1 pyahocorasick-2.1.0 pybind11-2.13.6 pyonmttok-1.37.1 rapidfuzz-3.10.0 sacrebleu-2.4.3 torch-2.2.2 triton-2.2.0 waitress-3.0.0\n"]}]},{"cell_type":"markdown","source":["# Prepare Your Datasets\n","Please make sure you have completed the [first exercise](https://colab.research.google.com/drive/1rsFPnAQu9-_A6e2Aw9JYK3C8mXx9djsF?usp=sharing)."],"metadata":{"id":"vhgIdJn-cLqu"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbkmPMC_d4Mg","executionInfo":{"status":"ok","timestamp":1728044331091,"user_tz":-120,"elapsed":16763,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}},"outputId":"38067ccc-5ec7-45b3-92aa-b20053c5cdcc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"dWVOWYedzZ_G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728044333589,"user_tz":-120,"elapsed":278,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}},"outputId":"775afed3-d841-4999-84ba-035c79423d0a"},"source":["# Open the folder where you saved your prepapred datasets from the first exercise\n","# You might need to mount your Google Drive first\n","%cd /content/drive/MyDrive/nmt/\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/nmt\n","en-nl.txt.zip\t\t\t      Tatoeba.en-nl.en-filtered.en.subword.dev\n","LICENSE\t\t\t\t      Tatoeba.en-nl.en-filtered.en.subword.test\n","MT-Preparation\t\t\t      Tatoeba.en-nl.en-filtered.en.subword.train\n","README\t\t\t\t      Tatoeba.en-nl.nl\n","source.model\t\t\t      Tatoeba.en-nl.nl-filtered.nl\n","source.vocab\t\t\t      Tatoeba.en-nl.nl-filtered.nl.subword\n","target.model\t\t\t      Tatoeba.en-nl.nl-filtered.nl.subword.dev\n","target.vocab\t\t\t      Tatoeba.en-nl.nl-filtered.nl.subword.test\n","Tatoeba.en-nl.en\t\t      Tatoeba.en-nl.nl-filtered.nl.subword.train\n","Tatoeba.en-nl.en-filtered.en\t      Tatoeba.en-nl.xml\n","Tatoeba.en-nl.en-filtered.en.subword\n"]}]},{"cell_type":"markdown","metadata":{"id":"MPlmhd426B7l"},"source":["# Create the Training Configuration File\n","\n","The following config file matches most of the recommended values for the Transformer model [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762). As the current dataset is small, we reduced the following values:\n","* `train_steps` - for datasets with a few millions of sentences, consider using a value between 100000 and 200000, or more! Enabling the option `early_stopping` can help stop the training when there is no considerable improvement.\n","* `valid_steps` - 10000 can be good if the value `train_steps` is big enough.\n","* `warmup_steps` - obviously, its value must be less than `train_steps`. Try 4000 and 8000 values.\n","\n","Refer to [OpenNMT-py training parameters](https://opennmt.net/OpenNMT-py/options/train.html) for more details. If you are interested in further explanation of the Transformer model, you can check this article, [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)."]},{"cell_type":"code","metadata":{"id":"qbW7Xek6UDlY"},"source":["# Create the YAML configuration file\n","# On a regular machine, you can create it manually or with nano\n","# Note here we are using some smaller values because the dataset is small\n","# For larger datasets, consider increasing: train_steps, valid_steps, warmup_steps, save_checkpoint_steps, keep_checkpoint\n","\n","config = '''# config.yaml\n","\n","\n","## Where the samples will be written\n","save_data: run\n","\n","# Training files\n","data:\n","    corpus_1:\n","        path_src: Tatoeba.en-nl.en-filtered.en.subword.train\n","        path_tgt: Tatoeba.en-nl.nl-filtered.nl.subword.train\n","        transforms: [filtertoolong]\n","    valid:\n","        path_src: Tatoeba.en-nl.en-filtered.en.subword.dev\n","        path_tgt: Tatoeba.en-nl.nl-filtered.nl.subword.dev\n","        transforms: [filtertoolong]\n","\n","# Vocabulary files, generated by onmt_build_vocab\n","src_vocab: run/source.vocab\n","tgt_vocab: run/target.vocab\n","\n","# Vocabulary size - should be the same as in sentence piece\n","src_vocab_size: 50000\n","tgt_vocab_size: 50000\n","\n","# Filter out source/target longer than n if [filtertoolong] enabled\n","src_seq_length: 150\n","src_seq_length: 150\n","\n","# Tokenization options\n","src_subword_model: source.model\n","tgt_subword_model: target.model\n","\n","# Where to save the log file and the output models/checkpoints\n","log_file: train.log\n","save_model: models/model.fren\n","\n","# Stop training if it does not imporve after n validations\n","early_stopping: 4\n","\n","# Default: 5000 - Save a model checkpoint for each n\n","save_checkpoint_steps: 1000\n","\n","# To save space, limit checkpoints to last n\n","# keep_checkpoint: 3\n","\n","seed: 3435\n","\n","# Default: 100000 - Train the model to max n steps\n","# Increase to 200000 or more for large datasets\n","# For fine-tuning, add up the required steps to the original steps\n","train_steps: 3000\n","\n","# Default: 10000 - Run validation after n steps\n","valid_steps: 1000\n","\n","# Default: 4000 - for large datasets, try up to 8000\n","warmup_steps: 1000\n","report_every: 100\n","\n","# Number of GPUs, and IDs of GPUs\n","world_size: 1\n","gpu_ranks: [0]\n","\n","# Batching\n","bucket_size: 262144\n","num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n","batch_type: \"tokens\"\n","batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n","valid_batch_size: 2048\n","max_generator_batches: 2\n","accum_count: [4]\n","accum_steps: [0]\n","\n","# Optimization\n","model_dtype: \"fp16\"\n","optim: \"adam\"\n","learning_rate: 2\n","# warmup_steps: 8000\n","decay_method: \"noam\"\n","adam_beta2: 0.998\n","max_grad_norm: 0\n","label_smoothing: 0.1\n","param_init: 0\n","param_init_glorot: true\n","normalization: \"tokens\"\n","\n","# Model\n","encoder_type: transformer\n","decoder_type: transformer\n","position_encoding: true\n","enc_layers: 6\n","dec_layers: 6\n","heads: 8\n","hidden_size: 512\n","word_vec_size: 512\n","transformer_ff: 2048\n","dropout_steps: [0]\n","dropout: [0.1]\n","attention_dropout: [0.1]\n","'''\n","\n","with open(\"config.yaml\", \"w+\") as config_yaml:\n","  config_yaml.write(config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vsL4zycvLMUx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728044937852,"user_tz":-120,"elapsed":272,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}},"outputId":"7dad363c-c8eb-429c-d73d-4a399384b343"},"source":["# [Optional] Check the content of the configuration file\n","!cat config.yaml"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# config.yaml\n","\n","\n","## Where the samples will be written\n","save_data: run\n","\n","# Training files\n","data:\n","    corpus_1:\n","        path_src: Tatoeba.en-nl.en-filtered.en.subword.train\n","        path_tgt: Tatoeba.en-nl.nl-filtered.nl.subword.train\n","        transforms: [filtertoolong]\n","    valid:\n","        path_src: Tatoeba.en-nl.en-filtered.en.subword.dev\n","        path_tgt: Tatoeba.en-nl.nl-filtered.nl.subword.dev\n","        transforms: [filtertoolong]\n","\n","# Vocabulary files, generated by onmt_build_vocab\n","src_vocab: run/source.vocab\n","tgt_vocab: run/target.vocab\n","\n","# Vocabulary size - should be the same as in sentence piece\n","src_vocab_size: 50000\n","tgt_vocab_size: 50000\n","\n","# Filter out source/target longer than n if [filtertoolong] enabled\n","src_seq_length: 150\n","src_seq_length: 150\n","\n","# Tokenization options\n","src_subword_model: source.model\n","tgt_subword_model: target.model\n","\n","# Where to save the log file and the output models/checkpoints\n","log_file: train.log\n","save_model: models/model.fren\n","\n","# Stop training if it does not imporve after n validations\n","early_stopping: 4\n","\n","# Default: 5000 - Save a model checkpoint for each n\n","save_checkpoint_steps: 1000\n","\n","# To save space, limit checkpoints to last n\n","# keep_checkpoint: 3\n","\n","seed: 3435\n","\n","# Default: 100000 - Train the model to max n steps \n","# Increase to 200000 or more for large datasets\n","# For fine-tuning, add up the required steps to the original steps\n","train_steps: 3000\n","\n","# Default: 10000 - Run validation after n steps\n","valid_steps: 1000\n","\n","# Default: 4000 - for large datasets, try up to 8000\n","warmup_steps: 1000\n","report_every: 100\n","\n","# Number of GPUs, and IDs of GPUs\n","world_size: 1\n","gpu_ranks: [0]\n","\n","# Batching\n","bucket_size: 262144\n","num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n","batch_type: \"tokens\"\n","batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n","valid_batch_size: 2048\n","max_generator_batches: 2\n","accum_count: [4]\n","accum_steps: [0]\n","\n","# Optimization\n","model_dtype: \"fp16\"\n","optim: \"adam\"\n","learning_rate: 2\n","# warmup_steps: 8000\n","decay_method: \"noam\"\n","adam_beta2: 0.998\n","max_grad_norm: 0\n","label_smoothing: 0.1\n","param_init: 0\n","param_init_glorot: true\n","normalization: \"tokens\"\n","\n","# Model\n","encoder_type: transformer\n","decoder_type: transformer\n","position_encoding: true\n","enc_layers: 6\n","dec_layers: 6\n","heads: 8\n","hidden_size: 512\n","word_vec_size: 512\n","transformer_ff: 2048\n","dropout_steps: [0]\n","dropout: [0.1]\n","attention_dropout: [0.1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"F0bcqYkEXhRY"},"source":["# Build Vocabulary\n","\n","For large datasets, it is not feasable to use all words/tokens found in the corpus. Instead, a specific set of vocabulary is extracted from the training dataset, usually betweeen 32k and 100k words. This is the main purpose of the vocabulary building step."]},{"cell_type":"code","metadata":{"id":"AuwltKp_VhnQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b49b5825-c4c8-474f-d44b-f800e8cdba5c","executionInfo":{"status":"ok","timestamp":1728044942803,"user_tz":-120,"elapsed":283,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}}},"source":["# Find the number of CPUs/cores on the machine\n","!nproc --all"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n"]}]},{"cell_type":"code","metadata":{"id":"P2GV1PgyUsJr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"52486c0e-006a-4571-fe97-494866370543","executionInfo":{"status":"ok","timestamp":1728045027067,"user_tz":-120,"elapsed":5572,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}}},"source":["# Build Vocabulary\n","\n","# -config: path to your config.yaml file\n","# -n_sample: use -1 to build vocabulary on all the segment in the training dataset\n","# -num_threads: change it to match the number of CPUs to run it faster\n","\n","!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Corpus corpus_1's weight should be given. We default it to 1 for you.\n","[2024-10-04 12:30:24,332 INFO] Counter vocab from -1 samples.\n","[2024-10-04 12:30:24,332 INFO] n_sample=-1: Build vocab on full datasets.\n","[2024-10-04 12:30:26,504 INFO] * Transform statistics for corpus_1(50.00%):\n","\t\t\t* FilterTooLongStats(filtered=1)\n","\n","[2024-10-04 12:30:26,514 INFO] * Transform statistics for corpus_1(50.00%):\n","\t\t\t* FilterTooLongStats(filtered=2)\n","\n","[2024-10-04 12:30:26,560 INFO] Counters src: 12911\n","[2024-10-04 12:30:26,561 INFO] Counters tgt: 16794\n"]}]},{"cell_type":"markdown","metadata":{"id":"ncWyNtxiO_Ov"},"source":["From the **Runtime menu** > **Change runtime type**, make sure that the \"**Hardware accelerator**\" is \"**GPU**\".\n"]},{"cell_type":"code","metadata":{"id":"TMMPeS-pSV8I","colab":{"base_uri":"https://localhost:8080/"},"outputId":"10a5f7a6-31e2-40d5-b59c-da5795f8b0c7","executionInfo":{"status":"ok","timestamp":1728045053730,"user_tz":-120,"elapsed":337,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}}},"source":["# Check if the GPU is active\n","!nvidia-smi -L"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla T4 (UUID: GPU-db46c46c-e2ff-fa50-9e0c-a98e4d2916da)\n"]}]},{"cell_type":"code","metadata":{"id":"_3rVQhd4NXNG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b3e11bd3-c41c-4e02-9315-37077b38cd0e","executionInfo":{"status":"ok","timestamp":1728045063460,"user_tz":-120,"elapsed":1819,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}}},"source":["# Check if the GPU is visable to PyTorch\n","\n","import torch\n","\n","print(torch.cuda.is_available())\n","print(torch.cuda.get_device_name(0))\n","\n","gpu_memory = torch.cuda.mem_get_info(0)\n","print(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","Tesla T4\n","Free GPU memory: 14999.0625 out of: 15102.0625\n"]}]},{"cell_type":"markdown","metadata":{"id":"8aCxETSnXcL-"},"source":["# Training\n","\n","Now, start training your NMT model! üéâ üéâ üéâ"]},{"cell_type":"code","source":["!rm -rf drive/MyDrive/nmt/models/"],"metadata":{"id":"HZd1o1kIb6Nv"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"prJCKA2CP-dl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728047679956,"user_tz":-120,"elapsed":2598700,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}},"outputId":"43d0e64a-fdbe-4d2c-bf38-cf30c40c689c"},"source":["# Train the NMT model\n","!onmt_train -config config.yaml"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2024-10-04 12:31:24,694 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n","[2024-10-04 12:31:24,695 INFO] Parsed 2 corpora from -data.\n","[2024-10-04 12:31:24,695 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n","[2024-10-04 12:31:24,757 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '.', '‚ñÅI', \"'\", '‚ñÅthe', '?', '‚ñÅto']\n","[2024-10-04 12:31:24,757 INFO] The decoder start token is: <s>\n","[2024-10-04 12:31:24,757 INFO] Building model...\n","[2024-10-04 12:31:26,214 INFO] Switching model to float32 for amp/apex_amp\n","[2024-10-04 12:31:26,215 INFO] Non quantized layer compute is fp16\n","[2024-10-04 12:31:26,503 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(12920, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding()\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): ModuleList(\n","      (0-5): 6 x TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(16800, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding()\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","    (transformer_layers): ModuleList(\n","      (0-5): 6 x TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (generator): Linear(in_features=512, out_features=16800, bias=True)\n",")\n","[2024-10-04 12:31:26,506 INFO] encoder: 25502720\n","[2024-10-04 12:31:26,507 INFO] decoder: 42405280\n","[2024-10-04 12:31:26,507 INFO] * number of parameters: 67908000\n","[2024-10-04 12:31:26,508 INFO] Trainable parameters = {'torch.float32': 67908000, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n","[2024-10-04 12:31:26,508 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n","[2024-10-04 12:31:26,508 INFO]  * src vocab size = 12920\n","[2024-10-04 12:31:26,508 INFO]  * tgt vocab size = 16800\n","[2024-10-04 12:31:26,884 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 1\n","[2024-10-04 12:31:26,885 INFO] Starting training on GPU: [0]\n","[2024-10-04 12:31:26,885 INFO] Start training loop and validate every 1000 steps...\n","[2024-10-04 12:31:26,885 INFO] Scoring with: ['filtertoolong']\n","[2024-10-04 12:31:28,492 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 2\n","[2024-10-04 12:31:30,020 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 3\n","[2024-10-04 12:31:31,786 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 4\n","[2024-10-04 12:32:45,675 INFO] Step 100/ 3000; acc: 17.9; ppl: 1611.0; xent: 7.4; lr: 0.00028; sents:  168993; bsz: 3200/3533/422; 16245/17936 tok/s;     79 sec;\n","[2024-10-04 12:33:06,590 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=11)\n","\n","[2024-10-04 12:33:06,591 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 5\n","[2024-10-04 12:33:08,184 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 6\n","[2024-10-04 12:33:11,743 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 7\n","[2024-10-04 12:33:12,743 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 8\n","[2024-10-04 12:34:09,377 INFO] Step 200/ 3000; acc: 37.6; ppl: 154.4; xent: 5.0; lr: 0.00056; sents:  165001; bsz: 3151/3524/413; 15057/16839 tok/s;    162 sec;\n","[2024-10-04 12:34:55,325 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=11)\n","\n","[2024-10-04 12:34:55,326 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 9\n","[2024-10-04 12:34:56,849 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 10\n","[2024-10-04 12:35:01,171 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 11\n","[2024-10-04 12:35:02,052 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 12\n","[2024-10-04 12:35:39,123 INFO] Step 300/ 3000; acc: 56.3; ppl:  41.4; xent: 3.7; lr: 0.00084; sents:  170047; bsz: 3235/3565/425; 14418/15888 tok/s;    252 sec;\n","[2024-10-04 12:36:44,133 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=12)\n","\n","[2024-10-04 12:36:44,134 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 13\n","[2024-10-04 12:36:48,689 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 14\n","[2024-10-04 12:36:49,616 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 15\n","[2024-10-04 12:36:53,868 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 16\n","[2024-10-04 12:37:06,631 INFO] Step 400/ 3000; acc: 71.4; ppl:  16.1; xent: 2.8; lr: 0.00112; sents:  170815; bsz: 3167/3546/427; 14474/16207 tok/s;    340 sec;\n","[2024-10-04 12:38:14,421 INFO] Step 500/ 3000; acc: 78.2; ppl:  10.5; xent: 2.4; lr: 0.00140; sents:  166810; bsz: 3183/3528/417; 18783/20818 tok/s;    408 sec;\n","[2024-10-04 12:38:38,352 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=12)\n","\n","[2024-10-04 12:38:38,353 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 17\n","[2024-10-04 12:38:39,291 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 18\n","[2024-10-04 12:38:43,372 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 19\n","[2024-10-04 12:39:45,668 INFO] Step 600/ 3000; acc: 82.4; ppl:   8.3; xent: 2.1; lr: 0.00168; sents:  169015; bsz: 3180/3555/423; 13942/15585 tok/s;    499 sec;\n","[2024-10-04 12:40:27,028 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=10)\n","\n","[2024-10-04 12:40:27,029 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 20\n","[2024-10-04 12:40:27,974 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 21\n","[2024-10-04 12:40:31,835 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 22\n","[2024-10-04 12:40:32,768 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 23\n","[2024-10-04 12:41:13,413 INFO] Step 700/ 3000; acc: 84.5; ppl:   7.5; xent: 2.0; lr: 0.00196; sents:  168529; bsz: 3188/3523/421; 14533/16059 tok/s;    587 sec;\n","[2024-10-04 12:42:20,310 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=11)\n","\n","[2024-10-04 12:42:20,310 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 24\n","[2024-10-04 12:42:21,249 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 25\n","[2024-10-04 12:42:25,766 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 26\n","[2024-10-04 12:42:27,343 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 27\n","[2024-10-04 12:42:46,363 INFO] Step 800/ 3000; acc: 85.9; ppl:   6.9; xent: 1.9; lr: 0.00224; sents:  168293; bsz: 3192/3546/421; 13739/15261 tok/s;    679 sec;\n","[2024-10-04 12:43:53,653 INFO] Step 900/ 3000; acc: 86.8; ppl:   6.6; xent: 1.9; lr: 0.00252; sents:  170040; bsz: 3173/3538/425; 18863/21032 tok/s;    747 sec;\n","[2024-10-04 12:44:11,813 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=12)\n","\n","[2024-10-04 12:44:11,814 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 28\n","[2024-10-04 12:44:16,361 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 29\n","[2024-10-04 12:44:17,378 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 30\n","[2024-10-04 12:44:21,580 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 31\n","[2024-10-04 12:45:25,892 INFO] Step 1000/ 3000; acc: 86.4; ppl:   6.7; xent: 1.9; lr: 0.00279; sents:  167533; bsz: 3177/3548/419; 13777/15388 tok/s;    839 sec;\n","[2024-10-04 12:45:27,782 INFO] valid stats calculation\n","                           took: 1.888861894607544 s.\n","[2024-10-04 12:45:27,784 INFO] Train perplexity: 22.4244\n","[2024-10-04 12:45:27,784 INFO] Train accuracy: 68.7622\n","[2024-10-04 12:45:27,784 INFO] Sentences processed: 1.68508e+06\n","[2024-10-04 12:45:27,784 INFO] Average bsz: 3185/3541/421\n","[2024-10-04 12:45:27,784 INFO] Validation perplexity: 18.8007\n","[2024-10-04 12:45:27,784 INFO] Validation accuracy: 70.9577\n","[2024-10-04 12:45:27,784 INFO] Model is improving ppl: inf --> 18.8007.\n","[2024-10-04 12:45:27,784 INFO] Model is improving acc: -inf --> 70.9577.\n","[2024-10-04 12:45:27,790 INFO] Saving checkpoint models/model.fren_step_1000.pt\n","[2024-10-04 12:46:47,413 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=12)\n","\n","[2024-10-04 12:46:47,414 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 32\n","[2024-10-04 12:46:51,507 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 33\n","[2024-10-04 12:46:52,455 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 34\n","[2024-10-04 12:47:36,229 INFO] Step 1100/ 3000; acc: 85.7; ppl:   6.8; xent: 1.9; lr: 0.00266; sents:  168863; bsz: 3209/3553/422; 9849/10905 tok/s;    969 sec;\n","[2024-10-04 12:48:37,368 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=10)\n","\n","[2024-10-04 12:48:37,369 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 35\n","[2024-10-04 12:48:41,256 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 36\n","[2024-10-04 12:48:42,631 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 37\n","[2024-10-04 12:48:47,663 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 38\n","[2024-10-04 12:49:08,656 INFO] Step 1200/ 3000; acc: 86.2; ppl:   6.6; xent: 1.9; lr: 0.00255; sents:  169832; bsz: 3206/3522/425; 13875/15243 tok/s;   1062 sec;\n","[2024-10-04 12:50:15,650 INFO] Step 1300/ 3000; acc: 87.1; ppl:   6.4; xent: 1.9; lr: 0.00245; sents:  166313; bsz: 3175/3541/416; 18956/21141 tok/s;   1129 sec;\n","[2024-10-04 12:50:31,784 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=12)\n","\n","[2024-10-04 12:50:31,785 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 39\n","[2024-10-04 12:50:33,420 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 40\n","[2024-10-04 12:50:37,777 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 41\n","[2024-10-04 12:50:38,714 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 42\n","[2024-10-04 12:51:45,951 INFO] Step 1400/ 3000; acc: 88.7; ppl:   6.0; xent: 1.8; lr: 0.00236; sents:  169252; bsz: 3173/3560/423; 14055/15768 tok/s;   1219 sec;\n","[2024-10-04 12:52:23,705 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=11)\n","\n","[2024-10-04 12:52:23,706 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 43\n","[2024-10-04 12:52:27,619 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 44\n","[2024-10-04 12:52:28,556 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 45\n","[2024-10-04 12:52:32,813 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 46\n","[2024-10-04 12:53:19,112 INFO] Step 1500/ 3000; acc: 90.1; ppl:   5.6; xent: 1.7; lr: 0.00228; sents:  168387; bsz: 3223/3520/421; 13838/15113 tok/s;   1312 sec;\n","[2024-10-04 12:54:17,881 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=12)\n","\n","[2024-10-04 12:54:17,882 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 47\n","[2024-10-04 12:54:18,825 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 48\n","[2024-10-04 12:54:23,705 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 49\n","[2024-10-04 12:54:25,552 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 50\n","[2024-10-04 12:54:48,671 INFO] Step 1600/ 3000; acc: 90.8; ppl:   5.5; xent: 1.7; lr: 0.00221; sents:  169017; bsz: 3158/3552/423; 14107/15865 tok/s;   1402 sec;\n","[2024-10-04 12:55:56,053 INFO] Step 1700/ 3000; acc: 92.0; ppl:   5.2; xent: 1.7; lr: 0.00214; sents:  169540; bsz: 3207/3549/424; 19041/21066 tok/s;   1469 sec;\n","[2024-10-04 12:56:10,196 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=12)\n","\n","[2024-10-04 12:56:10,196 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 51\n","[2024-10-04 12:56:14,819 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 52\n","[2024-10-04 12:56:16,110 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 53\n","[2024-10-04 12:57:24,775 INFO] Step 1800/ 3000; acc: 93.0; ppl:   5.0; xent: 1.6; lr: 0.00208; sents:  167823; bsz: 3192/3540/420; 14391/15961 tok/s;   1558 sec;\n","[2024-10-04 12:57:59,309 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=10)\n","\n","[2024-10-04 12:57:59,310 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 54\n","[2024-10-04 12:58:03,621 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 55\n","[2024-10-04 12:58:07,699 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 56\n","[2024-10-04 12:58:08,637 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 57\n","[2024-10-04 12:58:56,840 INFO] Step 1900/ 3000; acc: 93.5; ppl:   4.9; xent: 1.6; lr: 0.00203; sents:  170041; bsz: 3173/3544/425; 13787/15396 tok/s;   1650 sec;\n","[2024-10-04 12:59:53,535 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=11)\n","\n","[2024-10-04 12:59:53,535 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 58\n","[2024-10-04 12:59:54,883 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 59\n","[2024-10-04 12:59:58,842 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 60\n","[2024-10-04 12:59:59,771 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 61\n","[2024-10-04 13:00:25,451 INFO] Step 2000/ 3000; acc: 94.3; ppl:   4.8; xent: 1.6; lr: 0.00198; sents:  167692; bsz: 3179/3546/419; 14350/16009 tok/s;   1739 sec;\n","[2024-10-04 13:00:26,651 INFO] valid stats calculation\n","                           took: 1.198662519454956 s.\n","[2024-10-04 13:00:26,652 INFO] Train perplexity: 11.2446\n","[2024-10-04 13:00:26,652 INFO] Train accuracy: 79.4505\n","[2024-10-04 13:00:26,652 INFO] Sentences processed: 3.37184e+06\n","[2024-10-04 13:00:26,652 INFO] Average bsz: 3187/3542/421\n","[2024-10-04 13:00:26,652 INFO] Validation perplexity: 17.095\n","[2024-10-04 13:00:26,652 INFO] Validation accuracy: 73.1198\n","[2024-10-04 13:00:26,652 INFO] Model is improving ppl: 18.8007 --> 17.095.\n","[2024-10-04 13:00:26,652 INFO] Model is improving acc: 70.9577 --> 73.1198.\n","[2024-10-04 13:00:26,658 INFO] Saving checkpoint models/model.fren_step_2000.pt\n","[2024-10-04 13:01:42,527 INFO] Step 2100/ 3000; acc: 94.8; ppl:   4.7; xent: 1.5; lr: 0.00193; sents:  167894; bsz: 3201/3540/420; 16612/18369 tok/s;   1816 sec;\n","[2024-10-04 13:01:57,450 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=12)\n","\n","[2024-10-04 13:01:57,451 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 62\n","[2024-10-04 13:01:58,523 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 63\n","[2024-10-04 13:02:02,854 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 64\n","[2024-10-04 13:02:03,770 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 65\n","[2024-10-04 13:03:15,551 INFO] Step 2200/ 3000; acc: 95.4; ppl:   4.6; xent: 1.5; lr: 0.00188; sents:  170639; bsz: 3178/3540/427; 13666/15223 tok/s;   1909 sec;\n","[2024-10-04 13:03:49,198 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=12)\n","\n","[2024-10-04 13:03:49,198 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 66\n","[2024-10-04 13:03:53,120 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 67\n","[2024-10-04 13:03:54,084 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 68\n","[2024-10-04 13:04:43,667 INFO] Step 2300/ 3000; acc: 95.6; ppl:   4.5; xent: 1.5; lr: 0.00184; sents:  167281; bsz: 3193/3536/418; 14493/16051 tok/s;   1997 sec;\n","[2024-10-04 13:05:37,822 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=10)\n","\n","[2024-10-04 13:05:37,823 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 69\n","[2024-10-04 13:05:41,398 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 70\n","[2024-10-04 13:05:45,563 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 71\n","[2024-10-04 13:05:47,123 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 72\n","[2024-10-04 13:06:15,371 INFO] Step 2400/ 3000; acc: 95.7; ppl:   4.5; xent: 1.5; lr: 0.00180; sents:  169374; bsz: 3212/3548/423; 14009/15477 tok/s;   2088 sec;\n","[2024-10-04 13:07:22,217 INFO] Step 2500/ 3000; acc: 96.2; ppl:   4.4; xent: 1.5; lr: 0.00177; sents:  168219; bsz: 3157/3551/421; 18894/21248 tok/s;   2155 sec;\n","[2024-10-04 13:07:31,407 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=11)\n","\n","[2024-10-04 13:07:31,408 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 73\n","[2024-10-04 13:07:35,505 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 74\n","[2024-10-04 13:07:37,175 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 75\n","[2024-10-04 13:07:41,327 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 76\n","[2024-10-04 13:08:54,970 INFO] Step 2600/ 3000; acc: 96.5; ppl:   4.4; xent: 1.5; lr: 0.00173; sents:  171016; bsz: 3211/3553/428; 13848/15322 tok/s;   2248 sec;\n","[2024-10-04 13:09:25,649 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=12)\n","\n","[2024-10-04 13:09:25,650 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 77\n","[2024-10-04 13:09:26,862 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 78\n","[2024-10-04 13:09:30,919 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 79\n","[2024-10-04 13:09:31,859 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 80\n","[2024-10-04 13:10:23,970 INFO] Step 2700/ 3000; acc: 96.6; ppl:   4.3; xent: 1.5; lr: 0.00170; sents:  167741; bsz: 3164/3514/419; 14222/15795 tok/s;   2337 sec;\n","[2024-10-04 13:11:16,763 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=12)\n","\n","[2024-10-04 13:11:16,763 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 81\n","[2024-10-04 13:11:20,660 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 82\n","[2024-10-04 13:11:21,612 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 83\n","[2024-10-04 13:11:26,133 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 84\n","[2024-10-04 13:11:51,732 INFO] Step 2800/ 3000; acc: 96.8; ppl:   4.3; xent: 1.5; lr: 0.00167; sents:  169908; bsz: 3189/3556/425; 14534/16208 tok/s;   2425 sec;\n","[2024-10-04 13:12:58,311 INFO] Step 2900/ 3000; acc: 96.9; ppl:   4.3; xent: 1.5; lr: 0.00164; sents:  166483; bsz: 3189/3529/416; 19160/21205 tok/s;   2491 sec;\n","[2024-10-04 13:13:08,933 INFO] * Transform statistics for corpus_1(100.00%):\n","\t\t\t* FilterTooLongStats(filtered=12)\n","\n","[2024-10-04 13:13:08,934 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 85\n","[2024-10-04 13:13:12,984 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 86\n","[2024-10-04 13:13:13,911 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 87\n","[2024-10-04 13:14:29,861 INFO] Step 3000/ 3000; acc: 97.2; ppl:   4.2; xent: 1.4; lr: 0.00161; sents:  168482; bsz: 3200/3557/421; 13981/15541 tok/s;   2583 sec;\n","[2024-10-04 13:14:31,032 INFO] valid stats calculation\n","                           took: 1.169395923614502 s.\n","[2024-10-04 13:14:31,033 INFO] Train perplexity: 8.24154\n","[2024-10-04 13:14:31,033 INFO] Train accuracy: 85.0222\n","[2024-10-04 13:14:31,033 INFO] Sentences processed: 5.05887e+06\n","[2024-10-04 13:14:31,033 INFO] Average bsz: 3188/3542/422\n","[2024-10-04 13:14:31,033 INFO] Validation perplexity: 17.254\n","[2024-10-04 13:14:31,033 INFO] Validation accuracy: 74.1272\n","[2024-10-04 13:14:31,033 INFO] Stalled patience: 3/4\n","[2024-10-04 13:14:31,039 INFO] Saving checkpoint models/model.fren_step_3000.pt\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"4q3LPLJLoIha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For error debugging try:\n","# !dmesg -T"],"metadata":{"id":"XUYAvE8ffK2k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eShpS01j-Jcp"},"source":["# Translation\n","\n","Translation Options:\n","* `-model` - specify the last model checkpoint name; try testing the quality of multiple checkpoints\n","* `-src` - the subworded test dataset, source file\n","* `-output` - give any file name to the new translation output file\n","* `-gpu` - GPU ID, usually 0 if you have one GPU. Otherwise, it will translate on CPU, which would be slower.\n","* `-min_length` - [optional] to avoid empty translations\n","* `-verbose` - [optional] if you want to print translations\n","\n","Refer to [OpenNMT-py translation options](https://opennmt.net/OpenNMT-py/options/translate.html) for more details."]},{"cell_type":"code","metadata":{"id":"MbQEGTj4TybH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"94b3e634-3542-43bc-b4e5-bb8cf8a85427","executionInfo":{"status":"ok","timestamp":1728047796681,"user_tz":-120,"elapsed":28422,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}}},"source":["# Translate the \"subworded\" source file of the test dataset\n","# Change the model name, if needed.\n","!onmt_translate -model models/model.fren_step_3000.pt -src Tatoeba.en-nl.en-filtered.en.subword.test -output Tatoeba.nl.translated -gpu 0 -min_length 1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2024-10-04 13:16:11,081 INFO] Loading checkpoint from models/model.fren_step_3000.pt\n","[2024-10-04 13:16:12,742 INFO] Loading data into the model\n","[2024-10-04 13:16:35,956 INFO] PRED SCORE: -0.2448, PRED PPL: 1.28 NB SENTENCES: 5000\n","Time w/o python interpreter load/terminate:  24.9027578830719\n"]}]},{"cell_type":"code","metadata":{"id":"XHYihrgfIrIO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d21979c4-4ba8-4c84-f34f-df783f592d3c","executionInfo":{"status":"ok","timestamp":1728047817578,"user_tz":-120,"elapsed":285,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}}},"source":["# Check the first 5 lines of the translation file\n","!head -n 5 Tatoeba.nl.translated"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚ñÅWe ‚ñÅhebben ‚ñÅeen ‚ñÅgroter ‚ñÅprobleem ‚ñÅdan ‚ñÅwe ‚ñÅdachten .\n","‚ñÅTom ‚ñÅparkeer de ‚ñÅzijn ‚ñÅauto ‚ñÅin ‚ñÅde ‚ñÅbuurt .\n","‚ñÅIk ‚ñÅzal ‚ñÅin ‚ñÅBoston ‚ñÅvliegen .\n","‚ñÅMijn ‚ñÅbroertje ‚ñÅwou ‚ñÅdat ‚ñÅu ‚ñÅme ‚ñÅgeleend ‚ñÅhad , ‚ñÅzo als ‚ñÅik ‚ñÅdacht ‚ñÅdat ‚ñÅhij ‚ñÅterug ‚ñÅzal ‚ñÅzijn .\n","‚ñÅIk ‚ñÅga ‚ñÅer ‚ñÅniet ‚ñÅin .\n"]}]},{"cell_type":"code","metadata":{"id":"zRsJm6UET2C_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3229ba71-a291-4200-faa4-27a6a5fb2d90","executionInfo":{"status":"ok","timestamp":1728047856196,"user_tz":-120,"elapsed":5607,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}}},"source":["# If needed install/update sentencepiece\n","!pip3 install --upgrade -q sentencepiece\n","\n","# Desubword the translation file\n","!python3 MT-Preparation/subwording/3-desubword.py target.model Tatoeba.nl.translated"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Done desubwording! Output: Tatoeba.nl.translated.desubword\n"]}]},{"cell_type":"code","metadata":{"id":"ai4RhhGaKBp1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b978eafb-a593-4d19-9561-4515989efd5f","executionInfo":{"status":"ok","timestamp":1728047878027,"user_tz":-120,"elapsed":274,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}}},"source":["# Check the first 5 lines of the desubworded translation file\n","!head -n 5 Tatoeba.nl.translated.desubword"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["We hebben een groter probleem dan we dachten.\n","Tom parkeerde zijn auto in de buurt.\n","Ik zal in Boston vliegen.\n","Mijn broertje wou dat u me geleend had, zoals ik dacht dat hij terug zal zijn.\n","Ik ga er niet in.\n"]}]},{"cell_type":"code","metadata":{"id":"kOUWB4r3OFOV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"51bae7ae-d76a-47f4-8e66-4e5f54249542","executionInfo":{"status":"ok","timestamp":1728047889622,"user_tz":-120,"elapsed":640,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}}},"source":["# Desubword the target file (reference) of the test dataset\n","# Note: You might as well have split files *before* subwording during dataset preperation,\n","# but sometimes datasets have tokeniztion issues, so this way you are sure the file is really untokenized.\n","!python3 MT-Preparation/subwording/3-desubword.py target.model Tatoeba.en-nl.nl-filtered.nl.subword.test"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Done desubwording! Output: Tatoeba.en-nl.nl-filtered.nl.subword.test.desubword\n"]}]},{"cell_type":"code","metadata":{"id":"0jULN0MwOFeH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bc01cbb2-353b-413d-f2db-ff9ed6f612f5","executionInfo":{"status":"ok","timestamp":1728047894836,"user_tz":-120,"elapsed":270,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}}},"source":["# Check the first 5 lines of the desubworded reference\n","!head -n 5 Tatoeba.en-nl.nl-filtered.nl.subword.test.desubword"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["We hebben een groter probleem dan we dachten.\n","Tom parkeerde zijn auto dichtbij.\n","Ik ga naar Boston vliegen.\n","Mijn broertje wou dat stripverhaal dat je onlangs aan mij hebt geleend lezen, dus ik zal het teruggeven nadat hij het uitgelezen heeft.\n","Ik ga daar niet naar binnen.\n"]}]},{"cell_type":"markdown","metadata":{"id":"bHMumxqvLDDc"},"source":["# MT Evaluation\n","\n","There are several MT Evaluation metrics such as BLEU, TER, METEOR, COMET, BERTScore, among others.\n","\n","Here we are using BLEU. Files must be detokenized/desubworded beforehand."]},{"cell_type":"code","metadata":{"id":"w-9XGYnaJ-Nj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728047908222,"user_tz":-120,"elapsed":873,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}},"outputId":"fa522954-fd46-4fea-f943-6a0d8bda192b"},"source":["# Download the BLEU script\n","!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-10-04 13:18:27--  https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 957 [text/plain]\n","Saving to: ‚Äòcompute-bleu.py‚Äô\n","\n","\rcompute-bleu.py       0%[                    ]       0  --.-KB/s               \rcompute-bleu.py     100%[===================>]     957  --.-KB/s    in 0s      \n","\n","2024-10-04 13:18:28 (54.3 MB/s) - ‚Äòcompute-bleu.py‚Äô saved [957/957]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"rYDG0x0KLk_O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728047927829,"user_tz":-120,"elapsed":2494,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}},"outputId":"3dd0d805-1c01-4d14-9cda-52649db5173c"},"source":["# Install sacrebleu\n","!pip3 install sacrebleu"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.3)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.10.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.9.11)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n"]}]},{"cell_type":"code","metadata":{"id":"W3V3tZphTzK9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f6c0093d-772e-4f26-a15c-941f48e4dcaa","executionInfo":{"status":"ok","timestamp":1728047935260,"user_tz":-120,"elapsed":1125,"user":{"displayName":"Amirhossein Shahsavari","userId":"13717602693183585849"}}},"source":["# Evaluate the translation (without subwording)\n","!python3 compute-bleu.py Tatoeba.en-nl.nl-filtered.nl.subword.test.desubword Tatoeba.nl.translated.desubword"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reference 1st sentence: We hebben een groter probleem dan we dachten.\n","MTed 1st sentence: We hebben een groter probleem dan we dachten.\n","BLEU:  45.04064628160162\n"]}]},{"cell_type":"markdown","metadata":{"id":"IBi1PhRv4bX9"},"source":["# More Features and Directions to Explore\n","\n","Experiment with the following ideas:\n","* Icrease `train_steps` and see to what extent new checkpoints provide better translation, in terms of both BLEU and your human evaluation.\n","\n","* Check other MT Evaluation mentrics other than BLEU such as [TER](https://github.com/mjpost/sacrebleu#ter), [WER](https://blog.machinetranslation.io/compute-wer-score/), [METEOR](https://blog.machinetranslation.io/compute-bleu-score/#meteor), [COMET](https://github.com/Unbabel/COMET), and [BERTScore](https://github.com/Tiiiger/bert_score). What are the conceptual differences between them? Is there special cases for using a specific metric?\n","\n","* Continue training from the last model checkpoint using the `-train_from` option, only if the training stopped and you want to continue it. In this case, `train_steps` in the config file should be larger than the steps of the last checkpoint you train from.\n","```\n","!onmt_train -config config.yaml -train_from models/model.fren_step_3000.pt\n","```\n","\n","* **Ensemble Decoding:** During translation, instead of adding one model/checkpoint to the `-model` argument, add multiple checkpoints. For example, try the two last checkpoints. Does it improve quality of translation? Does it affect translation seepd?\n","\n","* **Averaging Models:** Try to average multiple models into one model using the [average_models.py](https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/bin/average_models.py) script, and see how this affects translation quality.\n","```\n","python3 average_models.py -models model_step_xxx.pt model_step_yyy.pt -output model_avg.pt\n","```\n","* **Release the model:** Try this command and see how it reduce the model size.\n","```\n","onmt_release_model --model \"model.pt\" --output \"model_released.pt\n","```\n","* **Use CTranslate2:** For efficient translation, consider using [CTranslate2](https://github.com/OpenNMT/CTranslate2), a fast inference engine. Check out an [example](https://gist.github.com/ymoslem/60e1d1dc44fe006f67e130b6ad703c4b).\n","\n","* **Work on low-resource languages:** Find out more details about [how to train NMT models for low-resource languages](https://blog.machinetranslation.io/low-resource-nmt/).\n","\n","* **Train a multilingual model:** Find out helpful notes about [training multilingual models](https://blog.machinetranslation.io/multilingual-nmt).\n","\n","* **Publish a demo:** Show off your work through a [simple demo with CTranslate2 and Streamlit](https://blog.machinetranslation.io/nmt-web-interface/).\n"]}]}
